# Formative Assessment in Science

## Introduction

Formative assessment—often termed Assessment for Learning (AfL)—represents the most powerful lever teachers have for improving pupil outcomes in science education. Unlike summative assessment, which evaluates learning at the end of a unit or course, formative assessment operates continuously throughout instruction, providing real-time feedback that shapes teaching decisions and guides pupil progress. In science education specifically, formative assessment takes on particular significance because scientific understanding is constructed incrementally, with misconceptions frequently lurking beneath surface-level comprehension.

The seminal work of Black and Wiliam (1998), *Inside the Black Box*, established that formative assessment practices could produce learning gains equivalent to raising pupil achievement by one or two grades. Their research demonstrated that effective formative assessment is not merely about more frequent testing, but involves fundamental shifts in classroom culture, pupil-teacher relationships, and the locus of control over learning. In science classrooms, where abstract concepts and counter-intuitive phenomena abound, these principles take on heightened importance.

This knowledge file explores the theoretical foundations, practical strategies, and subject-specific applications of formative assessment in science education across the primary and secondary phases. It examines diagnostic questioning techniques, the design and deployment of conceptual probes, whole-class response strategies, and the integration of formative assessment into the fabric of science instruction.

## Theoretical Foundations of Formative Assessment

### Defining Formative Assessment

Formative assessment encompasses all activities undertaken by teachers—and by their pupils—that provide information to be used as feedback to modify teaching and learning activities. This definition, originating from Black and Wiliam's comprehensive review, emphasises the *formative function* of assessment rather than any particular instrument or timing. A quiz used solely to generate marks is summative; the same quiz used to identify misconceptions and adjust subsequent instruction is formative.

In science education, this distinction proves particularly crucial. A multiple-choice question about photosynthesis might reveal that 70% of pupils selected the correct answer, but the formative value lies in analysing *which* distractors attracted the remaining 30%. If pupils consistently confuse the roles of chlorophyll and chloroplasts, this insight shapes the teacher's next explanation in ways that raw percentage scores cannot.

### The Five Key Strategies

Black and Wiliam's subsequent work (2004) identified five essential strategies that characterise effective formative assessment:

**1. Clarifying and sharing learning intentions and criteria for success.** In science, this means making explicit not just *what* pupils will learn, but what quality understanding looks like. Rather than stating "We are learning about electricity," effective practice involves sharing success criteria such as "I can explain why a bulb lights in one circuit but not another, using the model of current as flow" or "I can identify which variables need controlling in a fair test."

**2. Engineering effective classroom discussions, questions, and learning tasks.** This strategy lies at the heart of science formative assessment. Teachers must design questions and activities that reveal underlying thinking, not merely surface recall. A question asking pupils to predict what will happen when a magnet is moved near a compass reveals far more about conceptual understanding than one asking them to name the poles of a magnet.

**3. Providing feedback that moves learners forward.** Feedback in science must be actionable and specific. Comments like "Good work" or even "Check your understanding of forces" prove less effective than precise guidance: "Your diagram shows the force arrow pointing in the direction of motion. Consider: what happens to the speed of the object if the force and motion are in the same direction? What if they oppose each other?"

**4. Activating students as learning resources for one another.** Peer assessment and collaborative learning take on particular power in science, where articulating reasoning and challenging classmates' ideas approximates the discourse of scientific communities. When pupils evaluate each other's experimental designs or critique the evidence cited in an argument, they internalise scientific criteria for quality.

**5. Activating students as owners of their own learning.** Self-assessment against clear criteria helps pupils develop metacognitive awareness of their scientific understanding. Pupils who can accurately assess whether they truly understand a concept—or merely recognise familiar terminology—are better positioned to direct their own study and seek appropriate support.

## Diagnostic Questioning in Science

### The Logic of Diagnostic Questions

Diagnostic questions—sometimes termed hinge questions—are carefully crafted multiple-choice items where each incorrect response corresponds to a specific misconception or error pattern. Unlike conventional multiple-choice questions designed primarily to test recall, diagnostic questions function as conceptual X-rays, revealing the underlying mental models pupils hold.

The design of effective diagnostic questions requires deep subject knowledge and awareness of common pupil thinking patterns. Consider a typical question about thermal equilibrium:

*Two blocks of metal sit in a room. Block A is at 40°C; Block B is at 20°C. After several hours, what happens?*

- A) Both blocks will be at 30°C
- B) Both blocks will reach the same temperature, but we cannot say what temperature
- C) Block A stays at 40°C; Block B stays at 20°C
- D) Block A warms up; Block B cools down further

Each option reveals distinct reasoning. Option A suggests the pupil conflates thermal equilibrium with arithmetic averaging. Option C indicates a substance-based view of temperature as an intrinsic property. Option D reveals confusion about the direction of heat flow. Only Option B demonstrates correct understanding of thermal equilibrium with environmental temperature.

### Question Design Principles

Effective diagnostic questions in science adhere to several design principles:

**Plausible distractors based on research.** The incorrect options should represent genuine misconceptions documented in science education research, not random errors. Sources such as the *Alternative Frameworks* literature, the *Children's Learning in Science* project, and more recent studies of pupil conceptions provide rich sources of authentic alternative ideas.

**Single-concept focus.** Each question should target one conceptual domain. Questions that simultaneously test understanding of evaporation and condensation, for instance, make it impossible to determine which concept causes difficulty.

**No-calculator simplicity.** The cognitive demand should focus on conceptual understanding rather than procedural calculation. If pupils fail a question because they cannot divide, the diagnostic value is compromised.

**No-guesswork construction.** Pupils should not be able to eliminate options through test-wise strategies. Options like "All of the above" or "None of the above" should be avoided, as should options that are obviously longer or shorter than others.

### Deployment Strategies

Diagnostic questions prove most powerful when integrated into instruction at natural decision points—moments where the teacher must determine whether to proceed or reteach. These hinge moments might occur after introducing a new concept, before beginning a practical investigation, or following a homework assignment.

The *show-me* board technique—where all pupils display their responses simultaneously using mini whiteboards or digit cards—enables the teacher to scan the entire class in seconds. This whole-class visibility transforms assessment from a sampling activity (questioning individual pupils) to a census (seeing everyone's thinking). When the teacher observes that 40% of pupils have selected a particular misconception, this triggers immediate instructional adaptation rather than proceeding with the planned activity.

Digital tools have expanded the possibilities for diagnostic questioning. Platforms such as Socrative, Kahoot, and Microsoft Forms enable rapid collection and visualisation of responses. Some systems automatically cluster pupils by response pattern, facilitating targeted groupings for subsequent instruction. However, the pedagogical thinking underlying question design remains more important than the technological wrapper.

## Conceptual Probes and Assessment Items

### Understanding Conceptual Probes

Conceptual probes extend the logic of diagnostic questions into more extended formats, including prediction-observation-explanation tasks, concept cartoons, and focused listing activities. These probes are designed to elicit the reasoning behind pupil responses, not merely capture the response itself.

The prediction-observation-explanation (POE) strategy, developed by White and Gunstone (1992), exemplifies this approach. Pupils first predict what will happen in a phenomenon, then observe the actual outcome, and finally reconcile any discrepancy through explanation. In a typical POE task about electrical circuits, pupils might predict whether bulbs will light in various circuit configurations, observe the actual behaviour, and then explain any surprises. The prediction phase reveals existing mental models; the explanation phase tracks conceptual change.

Concept cartoons, developed by Keogh and Naylor, present pupils with visual scenarios showing characters expressing different viewpoints about a scientific phenomenon. A concept cartoon about evaporation might show three children debating whether water disappears from a puddle on a hot day. Pupils identify which viewpoint they agree with and explain their reasoning, revealing their understanding of the particulate nature of matter and the phase change process.

### Subject-Specific Probe Examples

**Physics: Force and Motion**

The *Force Concept Inventory* and related instruments have documented persistent misconceptions about force and motion. Conceptual probes in this domain often focus on the relationship between force and motion. A classic probe presents a ball thrown upward: at the instant it reaches its highest point, what forces act upon it? Pupils who identify only gravity demonstrate Newtonian understanding; those who suggest an upward "force of the throw" reveal Aristotelian intuitions persisting despite instruction.

**Chemistry: Conservation of Mass**

The *Priestley Test* and similar probes examine understanding of mass conservation during chemical change. Pupils are asked to predict what happens to the mass of a system when a chemical reaction produces a gas that escapes. Those who predict mass loss often hold a "matter is destroyed" conception or confuse mass with weight, failing to recognise that the gas remains part of the system unless it escapes the container.

**Biology: Photosynthesis and Respiration**

Conceptual probes in biology frequently address the relationship between photosynthesis and respiration. A common probe asks pupils to trace the journey of carbon atoms through a plant over 24 hours. Those who suggest carbon is "used up" in photosynthesis or that plants only respire at night reveal compartmentalised rather than integrated understanding of these complementary processes.

### Using Probes to Inform Instruction

The value of conceptual probes lies not in the individual responses but in the patterns that emerge across a class. When a probe reveals that the majority of pupils hold a particular misconception, this triggers specific instructional responses:

**Cognitive conflict approaches** present phenomena that contradict the misconception, creating productive discomfort that motivates conceptual change. Pupils who believe heavy objects fall faster might observe a demonstration showing that a heavy and light object fall at the same rate in the absence of air resistance.

**Bridging analogies** help pupils construct new understanding by connecting to more familiar domains. Understanding electrical circuits can be supported by analogies to water flow in pipes, provided pupils recognise the limitations of the analogy.

**Explicit metaconceptual discussion** brings misconceptions into the open for examination. Rather than simply telling pupils their idea is wrong, effective practice involves discussing why the misconception is attractive, when it seems to work, and why it fails in particular circumstances.

## Assessment for Learning Strategies in Practice

### The Role of Talk and Discussion

Dialogic teaching—characterised by purposeful questions, deep answers, and subsequent questions that build upon those answers—provides the foundation for formative assessment in science. Robin Alexander's work on dialogic teaching emphasises the importance of extended contributions from pupils, rather than the familiar initiation-response-feedback (IRF) pattern where teachers ask closed questions, pupils provide brief answers, and teachers evaluate.

In science classrooms, dialogic approaches enable teachers to probe the reasoning behind pupil contributions. When a pupil states that "plants get their food from the soil," a dialogic teacher does not simply correct this misconception but asks follow-up questions: "What do you mean by 'food'?" "How could we test whether plants need soil to grow?" "What happens to the mass of a plant grown only in water?" This sustained questioning reveals the depth and nature of the pupil's understanding.

The *Think-Pair-Share* routine structures participation so that all pupils engage with questions before any respond publicly. This thinking time improves the quality of responses and ensures that formative assessment samples the understanding of the whole class, not merely the most confident pupils.

### Written Work and Feedback

Written tasks in science provide opportunities for formative assessment when designed to reveal thinking processes rather than merely correct answers. Open-ended questions that ask pupils to explain phenomena, evaluate evidence, or design investigations prove more diagnostically valuable than worksheets requiring only factual recall.

Effective written feedback in science addresses specific conceptual issues rather than generic weaknesses. Rather than "More detail needed," effective feedback might state: "Your explanation mentions that 'particles move faster when heated.' Consider: do the particles themselves change, or is it their motion that changes? How does this relate to the conservation of mass?"

The *comment-only marking* approach, where grades or scores are withheld in favour of qualitative feedback, encourages pupils to focus on learning rather than performance. When pupils receive both marks and comments, research suggests they attend primarily to the marks, with comments going unread. By separating evaluative feedback from grading, teachers maximise the impact of their written guidance.

### Practical Work as Assessment Opportunity

Practical investigations in science provide rich contexts for formative assessment, revealing not only conceptual understanding but also procedural knowledge, scientific reasoning, and collaborative skills. The *predict-observe-explain* sequence structures practical work formatively by making pupils' thinking visible before observation confirms or contradicts predictions.

The *Ofsted science subject report* (2023) highlighted that practical work is most effective when it serves clear learning purposes rather than being deployed merely for engagement. Formative assessment of practical skills focuses on whether pupils can select appropriate equipment, identify variables, recognise sources of error, and draw valid conclusions from data.

Teacher circulation during practical work provides ongoing formative assessment opportunities. Rather than merely ensuring safety and equipment functionality, effective teachers use this time to question pupils about their procedures, probe their understanding of what they are observing, and identify groups who may need additional support or extension.

## Whole-Class Response Systems

### Mini Whiteboards and Show-Me Boards

Mini whiteboards represent perhaps the most versatile and low-cost formative assessment tool available to science teachers. Their value lies in the immediate visibility they provide of every pupil's thinking. When pupils hold up their responses simultaneously, the teacher can scan the room and instantly identify patterns of understanding and misconception.

Effective use of mini whiteboards requires attention to classroom routines. Pupils should be trained to write responses quickly, hold boards chest-high when the teacher signals, and avoid erasing responses until instructed. The teacher's scanning technique—systematically surveying the room rather than focusing on particular pupils—ensures that the assessment samples the whole class.

The *no hands up* rule complements mini whiteboard use. When pupils know that the teacher may ask anyone to explain their response, they remain engaged rather than opting out. This technique, associated with the *Assessment for Learning* programme, distributes attention and responsibility across the entire class.

### Digital Response Systems

Electronic voting systems and online platforms offer enhanced functionality for whole-class assessment. Beyond capturing responses, these systems can display distribution histograms, track individual pupil progress over time, and enable anonymised sharing of responses for discussion.

The *peer instruction* model developed by Eric Mazur for university physics instruction has been adapted for secondary science classrooms. Pupils first respond individually to a challenging conceptual question, then discuss their reasoning with peers, and finally respond again. Significant shifts between initial and final responses, visible in the system's display, indicate productive conceptual discussion.

However, digital systems also present risks. The time required for login and navigation can reduce the frequency of assessment moments. Technical failures can derail lessons. Perhaps most importantly, the visibility of response patterns can create performance pressure that inhibits honest disclosure of uncertainty. Teachers must weigh these trade-offs when selecting tools.

### Traffic Lighting and Confidence Signalling

Simple confidence-signalling techniques provide continuous formative assessment data during instruction. The *traffic light* system—where pupils display green (confident), amber (partial understanding), or red (confused)—enables teachers to gauge the pace and clarity of their instruction in real time.

More sophisticated variants ask pupils to signal confidence in specific aspects of their understanding: "Show me green if you could explain this to a friend, amber if you could answer a question about it, red if you would need help." This differentiation reveals pupils who feel generally confident but lack depth, or conversely, those who underestimate their competence.

## Addressing Misconceptions Through Formative Assessment

### The Persistence of Alternative Conceptions

Science education research has documented the remarkable persistence of misconceptions—or *alternative conceptions*—despite instruction. These intuitive ideas, often developed through everyday experience, frequently align with pre-scientific theories that dominated before the scientific revolution. The idea that motion requires force, that matter is continuous rather than particulate, or that evolution operates at the level of individuals rather than populations—all represent robust alternative conceptions that resist simple correction.

Formative assessment serves as the primary means of identifying these conceptions and monitoring their transformation. Because alternative conceptions often enable successful prediction in limited contexts, they can remain hidden unless assessment specifically probes conceptual understanding. A pupil who correctly predicts that a pushed object will stop moving may nevertheless hold an Aristotelian understanding of force and motion, revealed only by questions about objects moving in the absence of friction.

### Elicitation-Confrontation-Resolution Sequences

The conceptual change teaching model structures instruction around three phases, each supported by formative assessment:

**Elicitation** involves making pupils' existing ideas explicit through discussion, prediction tasks, or diagnostic questions. This phase recognises that pupils enter instruction with prior conceptions that will influence how they interpret new information. Formative assessment at this stage documents the range of ideas present in the classroom.

**Confrontation** creates cognitive conflict by presenting phenomena that contradict the alternative conception. The weight of a large object and a small object in a vacuum, the continued motion of objects in space, or the conservation of mass in sealed reactions—all provide disconfirming evidence. Formative assessment tracks whether pupils recognise the conflict and begin questioning their existing ideas.

**Resolution** involves constructing and consolidating the scientific conception. Formative assessment at this stage examines whether pupils can apply the new understanding to novel situations, not merely repeat definitions or recall the phenomena used during instruction.

### Differentiation Based on Assessment Data

Formative assessment enables responsive differentiation without requiring entirely separate activities for different pupils. When diagnostic questioning reveals three distinct response patterns, the teacher can form temporary groupings for targeted instruction: one group working on the foundational concept, another addressing a specific misconception, and a third extending to more complex applications.

The *checkpoint* model structures lessons around key assessment moments where pupils' readiness for the next phase is evaluated. Those demonstrating secure understanding proceed to extension activities; those requiring additional support receive targeted reteaching. This approach, associated with mastery learning models